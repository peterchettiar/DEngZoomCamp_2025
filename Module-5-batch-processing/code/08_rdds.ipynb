{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instanciating Spark\n",
    "\n",
    "Import the PySpark library and the `SparkSession` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/29 09:29:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"rdds_example\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading raw data into a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_green = spark.read.parquet(\"data/raw/green/*/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nSELECT\\n    EXTRACT(HOUR FROM lpep_pickup_datetime) AS hour,\\n    PULocationID AS zone,\\n\\n    SUM(total_amount) as revenue,\\n    COUNT(1) as number_records\\nFROM green\\nWHERE lpep_pickup_datetime >= '2020-01-01 00:00:00'\\nGROUP BY 1,2\\nORDER BY 1,2\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want to implement this query that we had used in the previouse session, but with RDDs\n",
    "\n",
    "\"\"\"\n",
    "SELECT\n",
    "    EXTRACT(HOUR FROM lpep_pickup_datetime) AS hour,\n",
    "    PULocationID AS zone,\n",
    "\n",
    "    SUM(total_amount) as revenue,\n",
    "    COUNT(1) as number_records\n",
    "FROM green\n",
    "WHERE lpep_pickup_datetime >= '2020-01-01 00:00:00'\n",
    "GROUP BY 1,2\n",
    "ORDER BY 1,2\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2019, 12, 18, 15, 52, 30), lpep_dropoff_datetime=datetime.datetime(2019, 12, 18, 15, 54, 39), store_and_fwd_flag='N', RatecodeID=1.0, PULocationID=264, DOLocationID=264, passenger_count=5.0, trip_distance=0.0, fare_amount=3.5, extra=0.5, mta_tax=0.5, tip_amount=0.01, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=4.81, payment_type=1.0, trip_type=1.0, congestion_surcharge=0.0),\n",
       " Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 1, 0, 45, 58), lpep_dropoff_datetime=datetime.datetime(2020, 1, 1, 0, 56, 39), store_and_fwd_flag='N', RatecodeID=5.0, PULocationID=66, DOLocationID=65, passenger_count=2.0, trip_distance=1.28, fare_amount=20.0, extra=0.0, mta_tax=0.0, tip_amount=4.06, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=24.36, payment_type=1.0, trip_type=2.0, congestion_surcharge=0.0),\n",
       " Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 1, 0, 41, 38), lpep_dropoff_datetime=datetime.datetime(2020, 1, 1, 0, 52, 49), store_and_fwd_flag='N', RatecodeID=1.0, PULocationID=181, DOLocationID=228, passenger_count=1.0, trip_distance=2.47, fare_amount=10.5, extra=0.5, mta_tax=0.5, tip_amount=3.54, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=15.34, payment_type=1.0, trip_type=1.0, congestion_surcharge=0.0),\n",
       " Row(VendorID=1, lpep_pickup_datetime=datetime.datetime(2020, 1, 1, 0, 52, 46), lpep_dropoff_datetime=datetime.datetime(2020, 1, 1, 1, 14, 21), store_and_fwd_flag='N', RatecodeID=1.0, PULocationID=129, DOLocationID=263, passenger_count=2.0, trip_distance=6.3, fare_amount=21.0, extra=3.25, mta_tax=0.5, tip_amount=0.0, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=25.05, payment_type=2.0, trip_type=1.0, congestion_surcharge=2.75),\n",
       " Row(VendorID=1, lpep_pickup_datetime=datetime.datetime(2020, 1, 1, 0, 19, 57), lpep_dropoff_datetime=datetime.datetime(2020, 1, 1, 0, 30, 56), store_and_fwd_flag='N', RatecodeID=1.0, PULocationID=210, DOLocationID=150, passenger_count=1.0, trip_distance=2.3, fare_amount=10.0, extra=0.5, mta_tax=0.5, tip_amount=0.0, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=11.3, payment_type=1.0, trip_type=1.0, congestion_surcharge=0.0)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To show the underlying rdd of the dataframe we run the `.rdd` method\n",
    "# And if we add the `.take(5)` method, it will return a list of rows in which the dataframe that we have is built\n",
    "# on top of - `Row` is a special object that is used for building dataframes\n",
    "df_green.rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Spark DataFrame into a RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only columns that are needed for the query\n",
    "rdd = df_green.select(\"lpep_pickup_datetime\",\"PULocationID\",\"total_amount\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(lpep_pickup_datetime=datetime.datetime(2019, 12, 18, 15, 52, 30), PULocationID=264, total_amount=4.81),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 1, 0, 45, 58), PULocationID=66, total_amount=24.36),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 1, 0, 41, 38), PULocationID=181, total_amount=15.34),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 1, 0, 52, 46), PULocationID=129, total_amount=25.05),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 1, 0, 19, 57), PULocationID=210, total_amount=11.3)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-creating query output on RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 1, 0, 45, 58), PULocationID=66, total_amount=24.36)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want to apply the `WHERE` statement from the query - we use the `.filter()` method for RDDs\n",
    "# The filter method applies a function and keeps only those that return True\n",
    "from datetime import datetime as dt\n",
    "rdd.filter(lambda row: row.lpep_pickup_datetime >= dt(2020,1,1)).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[((0, 66), (11809.39999999999, 557)),\n",
       " ((0, 210), (3354.7899999999972, 183)),\n",
       " ((1, 225), (2814.3199999999997, 112)),\n",
       " ((0, 82), (32809.18999999952, 2558)),\n",
       " ((0, 74), (37676.54999999988, 2704)),\n",
       " ((0, 134), (8018.680000000015, 597)),\n",
       " ((0, 42), (32414.179999999942, 2258)),\n",
       " ((0, 166), (15045.050000000041, 948)),\n",
       " ((0, 22), (1592.9499999999998, 58)),\n",
       " ((0, 130), (18727.24999999998, 1108))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So now the next step would be perform the `GROUP BY` statement - to do so we need to apply two methods.\n",
    "# First, we need to use the `map()` method to create key-value pairs, we use the transform function to do so.\n",
    "# Next, we want to reduce the number of records by the key and aggregate the values such that there is only one key\n",
    "# and a corresponding aggregated values in the final output\n",
    "\n",
    "def transform(row):\n",
    "    hour = row.lpep_pickup_datetime.hour\n",
    "    zone = row.PULocationID\n",
    "\n",
    "    revenue = row.total_amount\n",
    "    count = 1\n",
    "\n",
    "    # Return key-value pair\n",
    "    return ((hour,zone), (revenue, count))\n",
    "\n",
    "\"\"\"\n",
    "The reduceByKey() method in Spark groups records by their key and applies the provided\n",
    "function to aggregate values associated with the same key. The function operates pairwise, \n",
    "meaning it processes records iteratively rather than all at once.\n",
    "\n",
    "Aggregation is done in a distributed and efficient manner. Spark applies the function to pairs of values \n",
    "first within each partition, then merges the intermediate results across partitions.\n",
    "\n",
    "The function takes in two arguments at a time: the previously aggregated value and the next value, \n",
    "accumulating the result iteratively.\n",
    "\"\"\"\n",
    "def calculate(left_value, right_value):\n",
    "\n",
    "    left_amount, left_count = left_value\n",
    "    right_amount, right_count = right_value\n",
    "\n",
    "    output_amount = left_amount + right_amount\n",
    "    output_count = left_count + right_count\n",
    "\n",
    "    return (output_amount, output_count)\n",
    "    \n",
    "# Now to see the output of records after mapping\n",
    "rdd.filter(lambda row: row.lpep_pickup_datetime >= dt(2020,1,1)) \\\n",
    "    .map(transform).reduceByKey(calculate).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpacking the RDD into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have recreated our query output in RDD, we have to now unpack it into a DataFrame \n",
    "from collections import namedtuple\n",
    "\n",
    "# We need to perform this step as if we converted our output directly into a DF, \n",
    "# the shcema would not be there as well as column names\n",
    "RevenueRow = namedtuple('RevenueRow', ['hour', 'zone', 'revenue', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Helper function to help unpack the rdd\n",
    "def unwrap(row):\n",
    "    return RevenueRow(hour=row[0][0],\n",
    "                      zone=row[0][1], \n",
    "                      revenue=row[1][0],\n",
    "                      count=row[1][1]\n",
    "                      )\n",
    "\n",
    "df_result = rdd.filter(\n",
    "    lambda row: row.lpep_pickup_datetime >= dt(2020,1,1)).map(transform).reduceByKey(calculate).map(unwrap).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('hour', LongType(), True), StructField('zone', LongType(), True), StructField('revenue', DoubleType(), True), StructField('count', LongType(), True)])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------------------+-----+\n",
      "|hour|zone|           revenue|count|\n",
      "+----+----+------------------+-----+\n",
      "|   0|  66| 11809.39999999999|  557|\n",
      "|   0| 210|3354.7899999999972|  183|\n",
      "|   1| 225|2814.3199999999997|  112|\n",
      "|   0|  82| 32809.18999999952| 2558|\n",
      "|   0|  74| 37676.54999999988| 2704|\n",
      "|   0| 134| 8018.680000000015|  597|\n",
      "|   0|  42|32414.179999999942| 2258|\n",
      "|   0| 166|15045.050000000041|  948|\n",
      "|   0|  22|1592.9499999999998|   58|\n",
      "|   0| 130| 18727.24999999998| 1108|\n",
      "|   0| 226|11961.250000000007|  447|\n",
      "|   0| 146|           5843.85|  281|\n",
      "|   0| 190|            279.76|   18|\n",
      "|   1| 145| 3621.969999999998|  181|\n",
      "|   0|  70|            633.94|   30|\n",
      "|   1|  25| 8350.610000000017|  486|\n",
      "|   1|  49| 2498.299999999998|  145|\n",
      "|   1|  41|22661.169999999925| 1706|\n",
      "|   0| 198|            786.21|   28|\n",
      "|   0| 258|            548.23|   21|\n",
      "+----+----+------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_result.write.parquet(\"tmp/green_revenue\", mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
